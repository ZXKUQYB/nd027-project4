{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Notice : This is a user-created notebook, which is the development environment, but NOT part of the project\n",
    "# It serves as a playground to test Python code snippets before putting them into etl.py script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# https://knowledge.udacity.com/questions/137494\n",
    "# Unlike the same config items in dl.cfg, you should enclose your keys with single quotes when using this notebook\n",
    "# And always remember to **remove** these keys before sharing this notebook with others\n",
    "AWS_ACCESS_KEY_ID=''\n",
    "AWS_SECRET_ACCESS_KEY=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# If you created your EMR cluster in AWS regions other than the region where the Udacity S3 bucket resides, \n",
    "# make sure you delete them and recreate a new EMR cluster in us-west-2 region before trying to execute codes.\n",
    "#\n",
    "# There are many threads in the Udacity Knowledge which focus on this issue (some of them listed below):\n",
    "# https://knowledge.udacity.com/questions/613482\n",
    "# https://knowledge.udacity.com/questions/461243\n",
    "# https://knowledge.udacity.com/questions/245497\n",
    "#\n",
    "# You can also use some trivial methods to determine the exact AWS region of the Udacity S3 bucket\n",
    "# https://stackoverflow.com/questions/62996989/how-can-i-determine-the-region-for-a-public-aws-s3-bucket\n",
    "AWS_DEFAULT_REGION='us-west-2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#!aws configure set aws_access_key_id $AWS_ACCESS_KEY_ID\n",
    "#!aws configure set aws_secret_access_key $AWS_SECRET_ACCESS_KEY\n",
    "#!aws configure set default.region $AWS_DEFAULT_REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#!aws s3 ls \"s3://udacity-dend/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#input_data = \"s3a://udacity-dend\"\n",
    "#output_data = \"s3a://udacity-dend-bucket-asthlihzxhfc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.SparkSession.html\n",
    "spark = SparkSession.builder.config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\").appName(\"Project : Data Lake\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load only a small portion of the JSON dataset for development and testing purposes\n",
    "# WARNING : AWS free tier quota will be quickly consumed even if you load the entire dataset into EMR only for several times, \n",
    "# as it would result in excessive read (load into Spark DataFrames) and write (save as parquet files) operations\n",
    "path_song_data = \"s3a://{}:{}@udacity-dend/song_data/A/A/A/*.json\".format(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n",
    "\n",
    "# https://knowledge.udacity.com/questions/137494\n",
    "# The same discussion thread mentioned before also demonstrated how to embed inline AWS credentials in code, \n",
    "# like the one we used in this code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This Spark DataFrame will serve as the staging table of the song dataset\n",
    "df_staging_songs = spark.read.json(path_song_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: long (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_staging_songs.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+--------------------+----------------+------------------+---------+---------+------------------+--------------------+----+\n",
      "|         artist_id|artist_latitude|     artist_location|artist_longitude|       artist_name| duration|num_songs|           song_id|               title|year|\n",
      "+------------------+---------------+--------------------+----------------+------------------+---------+---------+------------------+--------------------+----+\n",
      "|ARTC1LV1187B9A4858|        51.4536|Goldsmith's Colle...|        -0.01802|The Bonzo Dog Band|301.40036|        1|SOAFBCP12A8C13CC7D|King Of Scurf (20...|1972|\n",
      "+------------------+---------------+--------------------+----------------+------------------+---------+---------+------------------+--------------------+----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_staging_songs.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist_id='ARTC1LV1187B9A4858', artist_latitude=51.4536, artist_location=\"Goldsmith's College, Lewisham, Lo\", artist_longitude=-0.01802, artist_name='The Bonzo Dog Band', duration=301.40036, num_songs=1, song_id='SOAFBCP12A8C13CC7D', title='King Of Scurf (2007 Digital Remaster)', year=1972)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_staging_songs.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_staging_songs.createOrReplaceTempView(\"staging_songs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-cte.html\n",
    "songs_table = spark.sql(\"\"\"\n",
    "WITH cte_song_id AS \n",
    "(\n",
    "    SELECT DISTINCT song_id \n",
    "    FROM staging_songs \n",
    "    WHERE song_id != ''\n",
    "    AND song_id IS NOT NULL\n",
    ")\n",
    "\n",
    "SELECT cte.song_id, ss.title, ss.artist_id, ss.year, ss.duration\n",
    "FROM cte_song_id cte\n",
    "INNER JOIN staging_songs ss ON cte.song_id = ss.song_id\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "artists_table = spark.sql(\"\"\"\n",
    "WITH cte_artist_id AS \n",
    "(\n",
    "    SELECT DISTINCT artist_id \n",
    "    FROM staging_songs \n",
    "    WHERE artist_id != ''\n",
    "    AND artist_id IS NOT NULL\n",
    ")\n",
    "\n",
    "SELECT cte.artist_id, ss.artist_name AS name, ss.artist_location, ss.artist_latitude, ss.artist_longitude \n",
    "FROM cte_artist_id cte\n",
    "INNER JOIN staging_songs ss ON cte.artist_id = ss.artist_id\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "path_songs_table = \"s3a://{}:{}@udacity-dend-bucket-asthlihzxhfc/songs_table\".format(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n",
    "path_artists_table = \"s3a://{}:{}@udacity-dend-bucket-asthlihzxhfc/artists_table\".format(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# https://www.upsolver.com/blog/apache-parquet-why-use\n",
    "# https://docs.aws.amazon.com/athena/latest/ug/convert-to-columnar.html\n",
    "songs_table.write.parquet(path_songs_table, mode='overwrite', partitionBy=[\"year\", \"artist_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html\n",
    "# https://sparkbyexamples.com/pyspark/pyspark-read-and-write-parquet-file/\n",
    "artists_table.write.parquet(path_artists_table, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Depending on the way we wrangle data, sometimes it may be useful if we save the staging table as parquet files\n",
    "#path_staging_songs = \"s3a://{}:{}@udacity-dend-bucket-asthlihzxhfc/staging_songs/\".format(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n",
    "#df_staging_songs.write.parquet(path_staging_songs, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#path_log_data = \"s3a://{}:{}@udacity-dend/log_data/*/*/*.json\".format(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n",
    "path_log_data = \"s3a://{}:{}@udacity-dend/log_data/2018/11/2018-11-30-events.json\".format(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This Spark DataFrame will serve as the staging table of the log dataset\n",
    "df_staging_events = spark.read.json(path_log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Filter the staging table, as we only need the log entries coming with specific page value\n",
    "df_staging_events = df_staging_events.filter(df_staging_events.page == 'NextSong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_staging_events.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+-----------------+------+-------------+--------------------+------+\n",
      "|       artist|     auth|firstName|gender|itemInSession|lastName|   length|level|            location|method|    page|     registration|sessionId|             song|status|           ts|           userAgent|userId|\n",
      "+-------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+-----------------+------+-------------+--------------------+------+\n",
      "|Stephen Lynch|Logged In|   Jayden|     M|            0|    Bell|182.85669| free|Dallas-Fort Worth...|   PUT|NextSong|1.540991795796E12|      829|Jim Henson's Dead|   200|1543537327796|Mozilla/5.0 (comp...|    91|\n",
      "+-------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+-----------------+------+-------------+--------------------+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_staging_events.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist='Stephen Lynch', auth='Logged In', firstName='Jayden', gender='M', itemInSession=0, lastName='Bell', length=182.85669, level='free', location='Dallas-Fort Worth-Arlington, TX', method='PUT', page='NextSong', registration=1540991795796.0, sessionId=829, song=\"Jim Henson's Dead\", status=200, ts=1543537327796, userAgent='Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.2; WOW64; Trident/6.0)', userId='91')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_staging_events.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_staging_events.createOrReplaceTempView(\"staging_events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-window.html\n",
    "# https://stackoverflow.com/questions/44923353/unable-to-select-top-10-records-per-group-in-sparksql\n",
    "users_table = spark.sql(\"\"\"\n",
    "WITH cte_user_id AS \n",
    "(\n",
    "    SELECT DISTINCT userId \n",
    "    FROM staging_events \n",
    "    WHERE userId != '' \n",
    "    AND userId IS NOT NULL\n",
    "),\n",
    "cte_staging_events_ranked AS\n",
    "(\n",
    "    SELECT DISTINCT userId, firstName, lastName, gender, level, ts, \n",
    "    ROW_NUMBER() OVER (PARTITION BY userId ORDER BY ts DESC) AS rank \n",
    "    FROM staging_events \n",
    "    WHERE userId != '' \n",
    "    AND userId IS NOT NULL \n",
    "    ORDER BY userId, rank\n",
    ")\n",
    "\n",
    "SELECT DISTINCT cte.userId, se.firstName, se.lastName, se.gender, se.level \n",
    "FROM cte_user_id cte\n",
    "INNER JOIN (SELECT * FROM cte_staging_events_ranked ser WHERE ser.rank = 1) se ON cte.userId = se.userId\n",
    "ORDER BY userId\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "path_users_table = \"s3a://{}:{}@udacity-dend-bucket-asthlihzxhfc/users_table\".format(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "users_table.write.parquet(path_users_table, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# https://spark.apache.org/docs/latest/sql-ref-functions-builtin.html#date-and-timestamp-functions\n",
    "# https://docs.databricks.com/sql/language-manual/functions/date_part.html\n",
    "# https://docs.databricks.com/sql/language-manual/functions/extract.html\n",
    "time_table = spark.sql(\"\"\"\n",
    "WITH cte_ts AS \n",
    "(\n",
    "    SELECT DISTINCT ts, to_timestamp(ts/1000) AS start_time \n",
    "    FROM staging_events \n",
    "    WHERE ts IS NOT NULL\n",
    ")\n",
    "\n",
    "SELECT start_time,\n",
    "EXTRACT(hour FROM start_time) AS hour, \n",
    "EXTRACT(day FROM start_time) AS day, \n",
    "EXTRACT(week FROM start_time) AS week, \n",
    "EXTRACT(month FROM start_time) AS month, \n",
    "EXTRACT(year FROM start_time) AS year, \n",
    "EXTRACT(dayofweek FROM start_time) AS weekday \n",
    "FROM cte_ts\n",
    "\"\"\")\n",
    "\n",
    "# https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.show.html\n",
    "# \"\"\").show(n=20, truncate=False, vertical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "path_time_table = \"s3a://{}:{}@udacity-dend-bucket-asthlihzxhfc/time_table\".format(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "time_table.write.parquet(path_time_table, mode='overwrite', partitionBy=[\"year\", \"month\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "path_songs_table_parquet = \"s3a://{}:{}@udacity-dend-bucket-asthlihzxhfc/songs_table\".format(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n",
    "path_artists_table_parquet = \"s3a://{}:{}@udacity-dend-bucket-asthlihzxhfc/artists_table\".format(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_par_songs = spark.read.parquet(path_songs_table_parquet)\n",
    "df_par_artists = spark.read.parquet(path_artists_table_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_par_songs.createOrReplaceTempView(\"parquet_songs_table\")\n",
    "df_par_artists.createOrReplaceTempView(\"parquet_artists_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# https://thecodersstop.com/spark/generate-sequential-and-unique-ids-in-a-spark-dataframe/\n",
    "# https://docs.databricks.com/sql/language-manual/functions/monotonically_increasing_id.html\n",
    "songplays_table = spark.sql(\"\"\"\n",
    "WITH cte_song_data AS \n",
    "(\n",
    "    SELECT s.song_id, s.title, s.artist_id, a.name, s.duration \n",
    "    FROM parquet_songs_table s \n",
    "    INNER JOIN parquet_artists_table a ON s.artist_id = a.artist_id \n",
    ")\n",
    "\n",
    "SELECT monotonically_increasing_id() as songplay_id, \n",
    "to_timestamp(se.ts/1000) AS start_time, \n",
    "se.userId, \n",
    "se.level, \n",
    "cte.song_id, \n",
    "cte.artist_id, \n",
    "se.sessionId, \n",
    "se.location, \n",
    "se.userAgent, \n",
    "EXTRACT(month FROM to_timestamp(se.ts/1000)) AS month, \n",
    "EXTRACT(year FROM to_timestamp(se.ts/1000)) AS year \n",
    "FROM staging_events se\n",
    "LEFT OUTER JOIN cte_song_data cte ON se.song = cte.title AND se.artist = cte.name AND se.length = cte.duration \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "path_songplays_table = \"s3a://{}:{}@udacity-dend-bucket-asthlihzxhfc/songplays_table\".format(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "songplays_table.write.parquet(path_songplays_table, mode='overwrite', partitionBy=[\"year\", \"month\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#real    61m13.878s\n",
    "#user    0m1.118s\n",
    "#sys     0m0.231s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
